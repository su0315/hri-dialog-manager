{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPu27cnSUsqs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-DEFdZqUsqz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CN_jM2swUsq1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "from stable_baselines3 import PPO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQSLyMt-Usq2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Action Model\n",
    "\n",
    "The action model predicts either a puzzle piece ID, to do nothing or to only forward the language group's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUbyLuuiUsq2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/X_DM.pickle', 'rb') as X_file:\n",
    "    model1_X = pickle.load(X_file)\n",
    "with open('data/y_DM.pickle', 'rb') as y_file:\n",
    "    model1_y = pickle.load(y_file)\n",
    "\n",
    "half = int(len(model1_X)/2)\n",
    "train1_X, test1_X, train1_y, test1_y = train_test_split(model1_X[:half], model1_y[:half], test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03g2EFTiUsq3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ActionEnv(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # ACTIONS\n",
    "        '''an action can be to pick up one out of no_pieces pieces, do nothing or to forward the lang team output'''\n",
    "        no_pieces = 15 + 1 + 1 \n",
    "        self.action_space = Discrete(no_pieces)\n",
    "        \n",
    "        # OBSERVATIONS \n",
    "        '''currently we only observe the start state'''\n",
    "        self.observation_space = Box(low=0, high=1, shape=(train1_X.shape[1],))\n",
    "\n",
    "        # POSSIBLE STATES\n",
    "        ''''here the start state is defined. since we used our synthetic data also for DL and testing the \n",
    "        rule based approach, we read in the data instead of creating them on the fly in every timestep.\n",
    "        since init is called only once, this we only do here to initiate .state and .p_gold. the start state \n",
    "        needs to be '''\n",
    "        self.state  = train1_X[i] # the output of previous groups at a given time\n",
    "        self.p_gold = train1_y[i] # the correct action (the name is a bit confusing, because inititally we wanted to train the model to choose the highest probability)\n",
    " \n",
    "        \n",
    "    def step(self, action):\n",
    "        '''Our actions do not cause a state transition, because we only have one state, the start state. \n",
    "        We only want to reward the correct action and punish a wrong one.\n",
    "        '''\n",
    "        if action == self.p_gold:\n",
    "            reward = 1 \n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        # Check if sequence is done\n",
    "        '''since we currently play single-round games the game is over after chosing an action and receiving the reward'''\n",
    "        done = True\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        ''' if we'd play a game that makes sense to visuzalize like cartpole this would be done here'''\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        '''here the start state is really defined.'''\n",
    "        global i\n",
    "        self.state = train1_X[i]\n",
    "        self.p_gold = train1_y[i]\n",
    "        i += 1\n",
    "        if i == len(train1_X):\n",
    "            i = 0\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jraDAbyUsq4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXVSsyMoUsq5",
    "outputId": "1c9a540d-e682-4498-8c1e-cc67d427a4b9",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "model1 = PPO(\"MlpPolicy\", ActionEnv(), verbose=1)\n",
    "model1.learn(total_timesteps=half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo-jeekIUsq7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdV6B2irUsq9",
    "outputId": "ac1e33c7-b93f-4932-f045-7c97250aa117",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(len(test1_X))):\n",
    "    obs = test1_X[i]\n",
    "    p_gold = test1_y[i]\n",
    "    pred, _ = model1.predict(obs)\n",
    "    if pred==p_gold:\n",
    "        correct += 1\n",
    "accuracy = correct/len(test1_X)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI7Da0QuUsq_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Uncertainty Model\n",
    "\n",
    "This model predicts whether the action model is more likely predicting a correct or incorrect action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqqXx41OUsrA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model2_y = np.ones((half,))\n",
    "for i in range(half, 2*half):\n",
    "    obs = model1_X[i]\n",
    "    p_gold = model1_y[i]\n",
    "    pred, _ = model1.predict(obs)\n",
    "    if pred==p_gold:\n",
    "        model2_y[i-half] = 0\n",
    "        \n",
    "train2_X, test2_X, train2_y, test2_y = train_test_split(model1_X[half:], model2_y, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTxxIuszUsrB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UncertaintyEnv(Env):\n",
    "    def __init__(self):\n",
    "        # ACTIONS\n",
    "        '''an action can be to predict either uncertainty (1) or certainty (0)'''\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        # OBSERVATION\n",
    "        '''currently we only observe the start state'''\n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, shape=(train2_X.shape[1],))\n",
    "\n",
    "        # POSSIBLE STATES\n",
    "        ''''here the start state is defined. since we used our synthetic data also for DL and testing the \n",
    "        rule based approach, we read in the data instead of creating them on the fly in every timestep.\n",
    "        since init is called only once, this we only do here to initiate .state and .p_gold. the start state \n",
    "        needs to be '''\n",
    "        self.state  = train2_X[i]\n",
    "        self.p_gold = train2_y[i]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''Our actions do not cause a state transition, because we only have one state, the start state. \n",
    "        We only want to reward the correct action and punish a wrong one.\n",
    "        '''        \n",
    "        if action == self.p_gold:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Check if sequence is done\n",
    "        '''since we currently play single-round games the game is over after chosing an action and receiving the reward'''\n",
    "\n",
    "        done = True\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        ''' if we'd play a game that makes sense to visuzalize like cartpole this would be done here'''\n",
    "\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        '''here the start state is really defined.'''\n",
    "        global i\n",
    "        \n",
    "        self.state = train2_X[i]\n",
    "        self.p_gold = train2_y[i]\n",
    "        i += 1\n",
    "        if i == len(train2_X):\n",
    "            i = 0\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfCGQa8iUsrB",
    "outputId": "7c922df4-8ddc-4fba-a975-a4094af12382",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "model2 = PPO(\"MlpPolicy\", UncertaintyEnv(), verbose=1)\n",
    "model2.learn(total_timesteps=half)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd6zcLFlUsrB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB7RAGQlUsrC",
    "outputId": "87bafc11-1ea9-43e3-bece-4a9acf953c42",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(len(test2_X))):\n",
    "    obs = test2_X[i]\n",
    "    p_gold = test2_y[i]\n",
    "    pred, _ = model2.predict(obs)\n",
    "    if pred==p_gold:\n",
    "        correct += 1\n",
    "accuracy2 = correct/len(test2_X)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYVNmLMbUsrC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model2.save('RL_uncertainty_model')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ed6c992ba7fdd0ca8e67b9e6bb1892b67ffbcbb0d6c7cc95f752a3faeb832f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
