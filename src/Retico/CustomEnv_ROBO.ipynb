{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPu27cnSUsqs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w-DEFdZqUsqz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 2.10.1 requires protobuf<5.0.0dev,>=3.20.1, but you have protobuf 3.19.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CN_jM2swUsq1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "from stable_baselines3 import PPO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQSLyMt-Usq2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Action Model\n",
    "\n",
    "The action model predicts either a puzzle piece ID, to do nothing or to only forward the language group's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KUbyLuuiUsq2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/X_DM.pickle', 'rb') as X_file:\n",
    "    model1_X = pickle.load(X_file)\n",
    "with open('data/y_DM.pickle', 'rb') as y_file:\n",
    "    model1_y = pickle.load(y_file)\n",
    "\n",
    "half = int(len(model1_X)/2)\n",
    "train1_X, test1_X, train1_y, test1_y = train_test_split(model1_X[:half], model1_y[:half], test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1 Environment Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "03g2EFTiUsq3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ActionEnv(Env):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # ACTIONS\n",
    "        '''an action can be to pick up one out of no_pieces pieces, do nothing or to forward the lang team output'''\n",
    "        no_pieces = 15 + 1 + 1 \n",
    "        self.action_space = Discrete(no_pieces)\n",
    "        \n",
    "        # OBSERVATIONS \n",
    "        '''currently we only observe the start state'''\n",
    "        self.observation_space = Box(low=0, high=1, shape=(train1_X.shape[1],))\n",
    "\n",
    "        # POSSIBLE STATES\n",
    "        ''''here the start state is defined. since we used our synthetic data also for DL and testing the \n",
    "        rule based approach, we read in the data instead of creating them on the fly in every timestep.\n",
    "        since init is called only once, this we only do here to initiate .state and .p_gold. the start state \n",
    "        needs to be '''\n",
    "        self.state  = train1_X[i] # the output of previous groups at a given time\n",
    "        self.p_gold = train1_y[i] # the correct action (the name is a bit confusing, because inititally we wanted to train the model to choose the highest probability)\n",
    " \n",
    "        \n",
    "    def step(self, action):\n",
    "        '''Our actions do not cause a state transition, because we only have one state, the start state. \n",
    "        We only want to reward the correct action and punish a wrong one.\n",
    "        '''\n",
    "        if action == self.p_gold:\n",
    "            reward = 1 \n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        # Check if sequence is done\n",
    "        '''since we currently play single-round games the game is over after chosing an action and receiving the reward'''\n",
    "        done = True\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        ''' if we'd play a game that makes sense to visuzalize like cartpole this would be done here'''\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        '''here the start state is really defined.'''\n",
    "        global i\n",
    "        self.state = train1_X[i]\n",
    "        self.p_gold = train1_y[i]\n",
    "        i += 1\n",
    "        if i == len(train1_X):\n",
    "            i = 0\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jraDAbyUsq4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# model1 = PPO.load('RL_action_model', env=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lXVSsyMoUsq5",
    "outputId": "1c9a540d-e682-4498-8c1e-cc67d427a4b9",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.96         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 724          |\n",
      "|    iterations           | 326          |\n",
      "|    time_elapsed         | 921          |\n",
      "|    total_timesteps      | 667648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063478574 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0139      |\n",
      "|    explained_variance   | 0.305        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0201       |\n",
      "|    n_updates            | 3250         |\n",
      "|    policy_gradient_loss | -0.00986     |\n",
      "|    value_loss           | 0.0815       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x173745f5c70>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "model1 = PPO(\"MlpPolicy\", ActionEnv(), verbose=1)\n",
    "model1.learn(total_timesteps=2/3*half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model1.save('RL_action_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo-jeekIUsq7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cdV6B2irUsq9",
    "outputId": "ac1e33c7-b93f-4932-f045-7c97250aa117",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333334/333334 [02:31<00:00, 2203.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9723700552598895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(len(test1_X))):\n",
    "    obs = test1_X[i]\n",
    "    p_gold = test1_y[i]\n",
    "    pred, _ = model1.predict(obs)\n",
    "    if pred==p_gold:\n",
    "        correct += 1\n",
    "accuracy = correct/len(test1_X)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI7Da0QuUsq_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Uncertainty Model\n",
    "\n",
    "This model predicts whether the action model is more likely predicting a correct or incorrect action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iqqXx41OUsrA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model2_y = np.ones((half,))\n",
    "for i in range(half, 2*half):\n",
    "    obs = model1_X[i]\n",
    "    p_gold = model1_y[i]\n",
    "    pred, _ = model1.predict(obs)\n",
    "    if pred==p_gold:\n",
    "        model2_y[i-half] = 0\n",
    "        \n",
    "train2_X, test2_X, train2_y, test2_y = train_test_split(model1_X[half:], model2_y, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iTxxIuszUsrB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class UncertaintyEnv(Env):\n",
    "    def __init__(self):\n",
    "        # ACTIONS\n",
    "        '''an action can be to predict either uncertainty (1) or certainty (0)'''\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        # OBSERVATION\n",
    "        '''currently we only observe the start state'''\n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, shape=(train2_X.shape[1],))\n",
    "\n",
    "        # POSSIBLE STATES\n",
    "        ''''here the start state is defined. since we used our synthetic data also for DL and testing the \n",
    "        rule based approach, we read in the data instead of creating them on the fly in every timestep.\n",
    "        since init is called only once, this we only do here to initiate .state and .p_gold. the start state \n",
    "        needs to be '''\n",
    "        self.state  = train2_X[i]\n",
    "        self.p_gold = train2_y[i]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''Our actions do not cause a state transition, because we only have one state, the start state. \n",
    "        We only want to reward the correct action and punish a wrong one.\n",
    "        '''        \n",
    "        if action == self.p_gold:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Check if sequence is done\n",
    "        '''since we currently play single-round games the game is over after chosing an action and receiving the reward'''\n",
    "\n",
    "        done = True\n",
    "        info = {}\n",
    "\n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        ''' if we'd play a game that makes sense to visuzalize like cartpole this would be done here'''\n",
    "\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        '''here the start state is really defined.'''\n",
    "        global i\n",
    "        \n",
    "        self.state = train2_X[i]\n",
    "        self.p_gold = train2_y[i]\n",
    "        i += 1\n",
    "        if i == len(train2_X):\n",
    "            i = 0\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# model2 = PPO.load('RL_uncertainty_model', env=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FfCGQa8iUsrB",
    "outputId": "7c922df4-8ddc-4fba-a975-a4094af12382",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1         |\n",
      "|    ep_rew_mean          | 0.96      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 815       |\n",
      "|    iterations           | 326       |\n",
      "|    time_elapsed         | 818       |\n",
      "|    total_timesteps      | 667648    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.24e-06 |\n",
      "|    explained_variance   | 0.223     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0486    |\n",
      "|    n_updates            | 3250      |\n",
      "|    policy_gradient_loss | -7.33e-10 |\n",
      "|    value_loss           | 0.0914    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x1737d36d5b0>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "model2 = PPO(\"MlpPolicy\", UncertaintyEnv(), verbose=1)\n",
    "model2.learn(total_timesteps=2/3*half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BYVNmLMbUsrC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model2.save('RL_uncertainty_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 333334/333334 [02:11<00:00, 2527.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972004055991888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(len(test2_X))):\n",
    "    obs = test2_X[i]\n",
    "    p_gold = test2_y[i]\n",
    "    pred, _ = model2.predict(obs)\n",
    "    if pred==p_gold:\n",
    "        correct += 1\n",
    "accuracy2 = correct/len(test2_X)\n",
    "print(accuracy2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "72ed6c992ba7fdd0ca8e67b9e6bb1892b67ffbcbb0d6c7cc95f752a3faeb832f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}